{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#x1F4D1; &nbsp; <span style=\"color:red\"> Reflections. Introduction To Reinforcement Learning. Lessons 9-10</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\">Links & Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reinforcement Learning: An Introduction http://incompleteideas.net/sutton/book/bookdraft2016aug.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "POMDP tutorials https://www.techfak.uni-bielefeld.de/~skopp/Lehre/STdKI_SS10/POMDP_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Making under Uncertainty. MDPs and POMDPs http://web.stanford.edu/~mykel/pomdps.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POMDPs https://people.eecs.berkeley.edu/~pabbeel/cs287-fa13/slides/pomdps.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  &#x1F916; &nbsp;  <span style=\"color:red\"> Lesson 9. Partially Observable MDPs</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A POMDP models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Formally, a POMDP is a 7-tuple ${\\displaystyle (S,A,T,R,\\Omega ,O,\\gamma )}$, where\n",
    "\n",
    "- ${\\displaystyle S}$ is a set of states,\n",
    "- ${\\displaystyle A}$ is a set of actions,\n",
    "- ${\\displaystyle T}$ is a set of conditional transition probabilities between states,\n",
    "- ${\\displaystyle R:S\\times A\\to \\mathbb {R} }$ is the reward function.\n",
    "- ${\\displaystyle \\Omega }$  is a set of observations,\n",
    "- ${\\displaystyle O}$ is a set of conditional observation probabilities, and\n",
    "- ${\\displaystyle \\gamma \\in [0,1]}$ is the discount factor.\n",
    "\n",
    "At each time period, the environment is in some state ${\\displaystyle s\\in S}$. \n",
    "\n",
    "The agent takes an action ${\\displaystyle a\\in A}$, which causes the environment to transition to state ${\\displaystyle s'}$ with probability ${\\displaystyle T(s'\\mid s,a)}$. \n",
    "\n",
    "At the same time, the agent receives an observation ${\\displaystyle o\\in \\Omega }$  which depends on the new state of the environment with probability ${\\displaystyle O(o\\mid s',a)}$. \n",
    "\n",
    "Finally, the agent receives a reward equal to ${\\displaystyle R(s,a)}$. \n",
    "\n",
    "Then the process repeats. \n",
    "\n",
    "The goal is for the agent to choose actions at each time step that maximize its expected future discounted reward: ${\\displaystyle E\\left[\\sum _{t=0}^{\\infty }\\gamma ^{t}r_{t}\\right]}$ ^. \n",
    "\n",
    "The discount factor ${\\displaystyle \\gamma }$ determines how much immediate rewards are favored over more distant rewards. When ${\\displaystyle \\gamma =0}$ the agent only cares about which action will yield the largest expected immediate reward; when ${\\displaystyle \\gamma =1}$ the agent cares about maximizing the expected sum of future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POMDP ≡ Continuous-Space Belief MDP (don’t get to observe the state itself, instead get sensory measurements)\n",
    "\n",
    "- A belief state is a distribution over states; in belief state b, probability b(s) is assigned to being in s\n",
    "- Policies in POMDPs are mappings from belief states to actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  &#x1F916; &nbsp;  <span style=\"color:red\"> Lesson 10. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
