{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#x1F4D1; &nbsp; <span style=\"color:darkblue\"> Reflections. Introduction To Reinforcement Learning. Lessons 1-2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  &#x1F578; &nbsp; Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial Intelligence: A Modern Approach \n",
    "http://aima.cs.berkeley.edu/\n",
    "#### Reinforcement Learning\n",
    "http://reinforcementlearning.ai-depot.com/\n",
    "#### Markov Decision Processes\n",
    "http://www.cs.rice.edu/~vardi/dag01/givan1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  &#x1F578; &nbsp;  Lesson 1.  Smoov & Curly's Bogus Journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3277600000000001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# Actions: U - Upper, R - right, L - left, D - down\n",
    "# U-U-R-R-R (5 times \"the right way\" with p=0.8) + \n",
    "# R-U-U-R-R(1 time \"the right way\" with p=0.8, 4 time \"the possible way\" with p=0.1)\n",
    "p_r, p_p = 0.8, 0.1\n",
    "math.pow(p_r, 5) + math.pow(p_p, 4)*p_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Modeling for decision making*** involves two distinct parties, one is the decision-maker and the other is the model-builder.\n",
    "\n",
    "Systems are formed with parts put together in a particular manner in order to pursuit an objective. The relationship between the parts determines what the system does and how it functions as a whole.\n",
    "\n",
    "A system that does not change is a static (i.e., deterministic) system. Many of the systems we are part of are dynamic systems, which are they change over time. \n",
    "\n",
    "In ***deterministic models***, a good decision is judged by the outcome alone. However, in probabilistic models, the decision-maker is concerned not only with the outcome value but also with the amount of risk each decision carries.\n",
    "\n",
    "Strategy of the ***Decision-Making Process***:\n",
    "\n",
    "### $\\mathit {\\color{darkblue} {Identify \\ the \\ decision \\ situation \\ and \\ understand \\ objectives \\mapsto Identify \\ alternatives \\mapsto Decompose \\ and \\ model \\ the \\ problem \\mapsto Choose \\ the \\ best\\ alternative \\mapsto Sensitivity \\ Analysis \\mapsto (Is \\ further \\ analysis \\ needed?) \\mapsto (if \\ NO) \\ Implement \\ the \\ chosen \\ alternative}}$\n",
    "\n",
    "Probabilistic Modeling: from Data to Information, from Information to Facts, and finally, from Facts to Knowledge. \n",
    "\n",
    "***Source of Errors*** in Decision Making: false assumptions, not having an accurate estimation of the probabilities, relying on expectations, difficulties in measuring the utility function, and forecast errors.\n",
    "\n",
    "The deficiencies about our knowledge of the future may be divided into three domains, each with rather murky boundaries:\n",
    "\n",
    "***Risk:*** One might be able to enumerate the outcomes and figure the probabilities. However, one must lookout for non-normal distributions, especially those with “fat tails”, as illustrated in the stock market by the rare events.\n",
    "\n",
    "***Uncertainty:*** One might be able to enumerate the outcomes but the probabilities are murky. Most of the time, the best one can do is to give a rank order to possible outcomes and then be careful that one has not omitted one of significance.\n",
    "\n",
    "Uncertainty is the fact of processes or business; probability is the guide for \"good\" events in processes or business.\n",
    "\n",
    "***Black Swans:*** The name comes from an Australian genetic anomaly. This is the domain of events which are either “extremely unlikely” or “inconceivable” but when they happen, and they do happen, they have serious consequences, usually bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Reinforcement Learning*** is a type of Machine Learning, and thereby also a branch of Artificial Intelligence. It allows machines and software agents to automatically determine the ideal behaviour within a specific context, in order to maximize its performance. Simple reward feedback is required for the agent to learn its behaviour; this is known as the reinforcement signal.\n",
    "\n",
    "There are three fundamental problems that RL must tackle: the exploration-exploitation tradeoff, the problem of delayed reward (credit assignment), and the need to generalize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ***Markov Decision Process (MDP)*** is just like a Markov Chain, except the transition matrix depends on the action taken by the decision maker (agent) at each time step. The agent receives a reward, which depends on the action and the state. The goal is to find a function, called a policy, which specifies which action to take in each state, so as to maximize some function (e.g., the mean or expected discounted sum) of the sequence of rewards. One can formalize this in terms of Bellman's equation, which can be solved iteratively using policy iteration. The unique fixed point of this equation is the optimal policy.\n",
    "\n",
    "A Markov Decision Process (MDP) model contains:\n",
    "    \n",
    "- A set of possible world states S\n",
    "- A set of possible actions A\n",
    "- A real valued reward function R(s,a)\n",
    "- A description T of each action’s effects in each state.\n",
    "\n",
    "***Markov Property***: the effects of an action taken in a state depend only on that state and not on the prior history.\n",
    "\n",
    "***Deterministic Actions:*** For each state and action we specify a new state.\n",
    "\n",
    "***Stochastic Actions:*** For each state and action we specify a probability distribution over next states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the transition matrix and reward functions. We are assuming states, actions and time are discrete. \n",
    "#### T(s,a,s') = Pr[S(t+1)=s' | S(t)=s, A(t)=a] \n",
    "#### R(s,a,s') = E[R(t+1)| S(t)=a, A(t)=a, S(t+1)=s'] \n",
    "Continuous MDPs can also be defined, but are usually solved by discretization.\n",
    "\n",
    "We define the value of performing action a in state s as follows:\n",
    "\n",
    "#### Q(s,a) = sum_s' T(s,a,s') [ R(s,a,s') + g V(s') ]  \n",
    "where 0 < g <= 1 is the amount by which we discount future rewards, and V(s) is overall value of state s, given by \n",
    "Bellman's equation:\n",
    "#### V(s) = max_a Q(s,a) = max_a sum_s' T(s,a,s') [ R(s,a,s') + g V(s) ]\n",
    "In words, the value of a state is the maximum expected reward we will get in that state, plus the expected discounted value of all possible successor states, s'. If we define\n",
    "#### R(s,a) = E[ R(s,a,s') ] = sum_{s'} T(s,a,s') R(s,a,s')\n",
    "the above equation simplifies to the more common form\n",
    "#### V(s) = max_a R(s,a) + sum_s' T(s,a,s') g V(s') \n",
    "which, for a fixed policy and a tabular (non-parametric) representation of the V/Q/T/R functions, can be rewritten in matrix-vector form as V = R + g T V. \n",
    "\n",
    "Solving these n simultaneous equations is called value determination (n is the number of states).\n",
    "\n",
    "If V/Q satisfies the Bellman equation, then the greedy policy\n",
    "#### p(s) = argmax_a Q(s,a)\n",
    "is optimal. If not, we can set p(s) to argmax_a Q(s,a) and re-evaluate V (and hence Q) and repeat. This is called policy iteration, and is guaranteed to converge to the unique optimal policy. \n",
    "\n",
    "The best theoretical upper bound on the number of iterations needed by policy iteration is exponential in n, but in practice, the number of steps is O(n). By formulating the problem as a linear program, it can be proved that one can find the optimal policy in polynomial time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
