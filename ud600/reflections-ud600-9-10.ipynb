{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#x1F4D1; &nbsp; <span style=\"color:red\"> Reflections. Introduction To Reinforcement Learning. Lessons 9-10</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\">Links & Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reinforcement Learning: An Introduction http://incompleteideas.net/sutton/book/bookdraft2016aug.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "POMDP tutorials https://www.techfak.uni-bielefeld.de/~skopp/Lehre/STdKI_SS10/POMDP_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POMDP solution methods https://www.techfak.uni-bielefeld.de/~skopp/Lehre/STdKI_SS10/POMDP_solution.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Making under Uncertainty. MDPs and POMDPs http://web.stanford.edu/~mykel/pomdps.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POMDPs https://people.eecs.berkeley.edu/~pabbeel/cs287-fa13/slides/pomdps.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte-Carlo Planning in Large POMDPs: http://papers.nips.cc/paper/4031-monte-carlo-planning-in-large-pomdps.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact Solutions of Interactive POMDPs Using Behavioral Equivalence\n",
    "https://www.cs.uic.edu/~piotr/papers/aamas06.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partially observable Markov decision process https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Survey of POMDP Solution Techniques https://www.cs.ubc.ca/~murphyk/Papers/pomdp.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Survey of POMDP Applications http://www.pomdp.org/papers/applications.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSOP: Efficient Point-Based POMDP Planning by Approximating Optimally Reachable Belief Spaces\n",
    "http://www.roboticsproceedings.org/rss04/p9.pdf    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Julia interface for defining, solving and simulating partially observable Markov decision processes\n",
    "http://juliapomdp.github.io/POMDPs.jl/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate POMDP Planning Software http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling Agents with Probabilistic Programs http://agentmodels.org/chapters/3d-reinforcement-learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WebPPL Documentation: http://webppl.readthedocs.io/en/master/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POMDPy: https://github.com/pemami4911/POMDPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pomdp-solve: http://www.pomdp.org/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  &#x1F916; &nbsp;  <span style=\"color:red\"> Lesson 9. Partially Observable MDPs</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A **POMDP** models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Formally, a POMDP is a 7-tuple ${\\displaystyle (S,A,T,R,\\Omega ,O,\\gamma )}$, where\n",
    "\n",
    "- ${\\displaystyle S}$ is a set of states,\n",
    "- ${\\displaystyle A}$ is a set of actions,\n",
    "- ${\\displaystyle T}$ is a set of conditional transition probabilities between states,\n",
    "- ${\\displaystyle R:S\\times A\\to \\mathbb {R} }$ is the reward function.\n",
    "- ${\\displaystyle \\Omega }$  is a set of observations,\n",
    "- ${\\displaystyle O}$ is a set of conditional observation probabilities, and\n",
    "- ${\\displaystyle \\gamma \\in [0,1]}$ is the discount factor.\n",
    "\n",
    "At each time period, the environment is in some state ${\\displaystyle s\\in S}$. \n",
    "\n",
    "The agent takes an action ${\\displaystyle a\\in A}$, which causes the environment to transition to state ${\\displaystyle s'}$ with probability ${\\displaystyle T(s'\\mid s,a)}$. \n",
    "\n",
    "At the same time, the agent receives an observation ${\\displaystyle o\\in \\Omega }$  which depends on the new state of the environment with probability ${\\displaystyle O(o\\mid s',a)}$. \n",
    "\n",
    "Finally, the agent receives a reward equal to ${\\displaystyle R(s,a)}$. \n",
    "\n",
    "Then the process repeats. \n",
    "\n",
    "The goal is for the agent to choose actions at each time step that maximize its expected future discounted reward: ${\\displaystyle E\\left[\\sum _{t=0}^{\\infty }\\gamma ^{t}r_{t}\\right]}$ ^. \n",
    "\n",
    "The discount factor ${\\displaystyle \\gamma }$ determines how much immediate rewards are favored over more distant rewards. When ${\\displaystyle \\gamma =0}$ the agent only cares about which action will yield the largest expected immediate reward; when ${\\displaystyle \\gamma =1}$ the agent cares about maximizing the expected sum of future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POMDP ≡ Continuous-Space Belief MDP (don’t get to observe the state itself, instead get sensory measurements)\n",
    "\n",
    "- A belief state is a distribution over states; in belief state b, probability b(s) is assigned to being in s\n",
    "- Policies in POMDPs are mappings from belief states to actions\n",
    "- More General POMDP: Model uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing belief states**\n",
    "\n",
    "- Begin with some initial belief state b prior to any observations\n",
    "- Compute new belief state b' based on current belief state b, action a, and observation o\n",
    "- b'(s') = P(s'| o, a, b)\n",
    "- b'(s') ∝ P(o | s', a, b)P(s'| a, b)\n",
    "- b'(s') ∝ O(o | s', a)P(s'| a, b)\n",
    "- b'(s') ∝ O(o | s', a) ${\\sum_s}$ P(s'| a, b, s)P(s | a, b)\n",
    "- b'(s') ∝ O(o | s, a) ${\\sum_s}$ T(s'| s, a)b(s)\n",
    "- Kalman filter: exact update of the belief state for linear dynamical systems\n",
    "- Particle filter: approximate update for general systems\n",
    "- For this case only considering discrete state problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Exact solution algorithms**\n",
    "  - for general POMDP it is PSPACE-hard\n",
    "  - value iteration\n",
    "  -  policy iteration: policy evaluation and policy improvement\n",
    "  \n",
    "- **Offline methods**\n",
    "  - Offline POMDP methods involve doing all or most of the computing prior to execution\n",
    "  - Practical method generally find only approximate solutions\n",
    "  - Some methods output policy in terms of alpha-vectors, others as finite-state controllers\n",
    "  - Some methods involve leveraging a factored representation\n",
    "  - Very active area of research\n",
    "  - Examples:\n",
    "    - QMDP value approximation\n",
    "    - Fast informed bound (FIB) value approximation\n",
    "    - Point-based value iteration methods\n",
    "  \n",
    "- **Online methods**\n",
    "  - Online methods determine the optimal policy by planning from the current belief state\n",
    "  - Many online methods use a depth-first tree-based search up to some horizon\n",
    "  - The belief states reachable from the current state is typically small compared to the full belief space\n",
    "  - Time complexity is exponential in the search depth\n",
    "  - Heuristics and brand-and-bound techniques allow search space to be pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Industrial Applications\n",
    "  - Machine Maintenance\n",
    "  - Structural Inspection\n",
    "  - Elevator Control Policies\n",
    "  - Fishery Industry\n",
    "- Scientific Applications\n",
    "  - Autonomous Robots\n",
    "    - interplanetary rovers\n",
    "    - deep-space navigation\n",
    "    - bomb disposal\n",
    "    - land-mine clearing\n",
    "    - toxic waste clean-up\n",
    "    - radioactive material handling\n",
    "    - deep-ocean exploration\n",
    "    - sewage/drainage network inspection and repair\n",
    "  - Behavioral Ecology\n",
    "  - Machine Vision\n",
    "- Business Applications\n",
    "  - Distributed Database Queries\n",
    "  - Marketing\n",
    "  - Questionnaire Design\n",
    "  - Corporate Policy\n",
    "- Military Applications\n",
    "  - Moving Target Search and Identification\n",
    "  - Rescue\n",
    "  - Weapon Allocation\n",
    "- Social Applications\n",
    "  - Development of Teaching Strategies\n",
    "  - Health Care Policymaking\n",
    "  - Advanced Medical Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current research directions\n",
    "\n",
    "- Fast! belief space planning\n",
    "- Multi-modal belief spaces\n",
    "- Physical experiments with the Raven surgical robot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quiz: POMDPs Generalize MDPs\n",
    "# Z = S, O(Z,S) = 1 for Z = S and O(Z,S) = 0 in another case. Then MDP == POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quiz: POMDP Example - 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  &#x1F916; &nbsp;  <span style=\"color:red\"> Lesson 10. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  &#x1F916; &nbsp;  <span style=\"color:red\"> Examples. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# POMDPs.jl\n",
    "using SARSOP, POMDPModels, POMDPToolbox\n",
    "\n",
    "# initialize problem and solver\n",
    "pomdp = TigerPOMDP() # from POMDPModels\n",
    "solver = SARSOPSolver() # from SARSOP\n",
    "\n",
    "# compute a policy\n",
    "policy = solve(solver, pomdp)\n",
    "\n",
    "#evaluate the policy\n",
    "belief_updater = updater(policy) # the default QMPD belief updater (discrete Bayesian filter)\n",
    "init_dist = initial_state_distribution(pomdp) # from POMDPModels\n",
    "hist = HistoryRecorder(max_steps=100) # from POMDPToolbox\n",
    "r = simulate(hist, pomdp, policy, belief_updater, init_dist) # run 100 step simulation\n",
    "'''\n",
    "''' '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WebPPL probabilistic programming online: http://webppl.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Posterior Sampling Reinforcement Learning (PSRL)\n",
    "http://agentmodels.org/chapters/3d-reinforcement-learning.html\n",
    "\n",
    "var observeState = function(state) { \n",
    "  return utility(state);\n",
    "};\n",
    "\n",
    "\n",
    "var makePSRLAgent = function(params, pomdp) {\n",
    "  var utility = params.utility;\n",
    "\n",
    "  // belief updating: identical to POMDP agent from Chapter 3c\n",
    "  var updateBelief = function(belief, observation, action){\n",
    "    return Infer({ model() {\n",
    "      var state = sample(belief);\n",
    "      var predictedNextState = transition(state, action);\n",
    "      var predictedObservation = observeState(predictedNextState);\n",
    "      condition(_.isEqual(predictedObservation, observation));\n",
    "      return predictedNextState;\n",
    "    }});\n",
    "  };\n",
    "\n",
    "  // this is the MDP agent from Chapter 3a\n",
    "  var act = dp.cache(\n",
    "    function(state) {\n",
    "      return Infer({ model() {\n",
    "        var action = uniformDraw(actions);\n",
    "        var eu = expectedUtility(state, action);\n",
    "        factor(1000 * eu);\n",
    "        return action;\n",
    "      }});\n",
    "    });\n",
    "\n",
    "  var expectedUtility = dp.cache(\n",
    "    function(state, action) {\n",
    "      return expectation(\n",
    "        Infer({ model() {\n",
    "          var u = utility(state, action);\n",
    "          if (state.manifestState.terminateAfterAction) {\n",
    "            return u;\n",
    "          } else {\n",
    "            var nextState = transition(state, action);\n",
    "            var nextAction = sample(act(nextState));\n",
    "            return u + expectedUtility(nextState, nextAction);\n",
    "          }\n",
    "        }}));\n",
    "    });\n",
    "\n",
    "  return { params, act, expectedUtility, updateBelief };\n",
    "};\n",
    "\n",
    "\n",
    "//NOTES:\n",
    "//We simulate with a single agent. Whenever the agent takes actions,\n",
    "//they are given a state (*believedState*) that contains the *latentState*.\n",
    "//Since utility(s,a) just depends on *latentState.rewardGrid*, this is equivalent to giving\n",
    "//them a reward function. If the agent is given the same starting state twice,\n",
    "//then their old plans are re-used due to caching.\n",
    "\n",
    "//(It might be a bit clearer to create the agent anew every episode. On the\n",
    "//other hand, the current code is elegant and exploits caching.)\n",
    "\n",
    "//People just reading this chapter will proably be confused by manifest/latent. \n",
    "//(We could bring the presentation closer to standard model-based RL\n",
    "//where R and T are unknown. But it's not clear it's worth doing so.)\n",
    "\n",
    "\n",
    "\n",
    "var simulatePSRL = function(startState, agent, numEpisodes) {\n",
    "  var act = agent.act;\n",
    "  var updateBelief = agent.updateBelief;\n",
    "  var priorBelief = agent.params.priorBelief;\n",
    "\n",
    "  var runSampledModelAndUpdate = function(state, priorBelief, numEpisodesLeft) {\n",
    "    var sampledState = sample(priorBelief);\n",
    "    var trajectory = simulateEpisode(state, sampledState, priorBelief, 'noAction');\n",
    "    var newBelief = trajectory[trajectory.length-1][2];\n",
    "    var newBelief2 = Infer({ model() {\n",
    "      return extend(state, {latentState : sample(newBelief).latentState });\n",
    "    }});\n",
    "    var output = [trajectory];\n",
    "\n",
    "    if (numEpisodesLeft <= 1){\n",
    "      return output;\n",
    "      } else {\n",
    "      return output.concat(runSampledModelAndUpdate(state, newBelief2,\n",
    "                                                    numEpisodesLeft-1));\n",
    "    }\n",
    "  };\n",
    "\n",
    "  var simulateEpisode = function(state, sampledState, priorBelief, action) {\n",
    "    var observation = observeState(state);\n",
    "    var belief = ((action === 'noAction') ? priorBelief : \n",
    "                  updateBelief(priorBelief, observation, action));\n",
    "\n",
    "    var believedState = extend(state, { latentState : sampledState.latentState });\n",
    "    var action = sample(act(believedState));\n",
    "    var output = [[state, action, belief]];\n",
    "\n",
    "    if (state.manifestState.terminateAfterAction){\n",
    "      return output;\n",
    "    } else {\n",
    "      var nextState = transition(state, action);\n",
    "      return output.concat(simulateEpisode(nextState, sampledState, belief, action));\n",
    "    }\n",
    "  };\n",
    "  return runSampledModelAndUpdate(startState, priorBelief, numEpisodes);\n",
    "};\n",
    "\n",
    "\n",
    "// Construct agent prior belief\n",
    "\n",
    "// Combine manifest (fully observed) state with prior on latent state\n",
    "var getPriorBelief = function(startManifestState, latentStateSampler){\n",
    "  return Infer({ model() {\n",
    "    return {\n",
    "      manifestState: startManifestState, \n",
    "      latentState: latentStateSampler()};\n",
    "  }});\n",
    "};\n",
    "\n",
    "// True reward function\n",
    "var trueLatentReward = {\n",
    "  rewardGrid : [\n",
    "      [ 0, 0, 0, 0],\n",
    "      [ 0, 0, 0, 0],\n",
    "      [ 0, 0, 0, 0],\n",
    "      [ 0, 0, 0, 1]\n",
    "    ]\n",
    "};\n",
    "\n",
    "// True start state\n",
    "var startState = {\n",
    "  manifestState: { \n",
    "    loc: [0, 0],\n",
    "    terminateAfterAction: false,\n",
    "    timeLeft: 8\n",
    "  },\n",
    "  latentState: trueLatentReward\n",
    "};\n",
    "\n",
    "// Agent prior on reward functions\n",
    "var latentStateSampler = function() {\n",
    "  var flat = getOneHotVector(16, randomInteger(16));\n",
    "  return { \n",
    "    rewardGrid : [\n",
    "      flat.slice(0,4), \n",
    "      flat.slice(4,8), \n",
    "      flat.slice(8,12), \n",
    "      flat.slice(12,16) ] \n",
    "  };\n",
    "};\n",
    "\n",
    "var priorBelief = getPriorBelief(startState.manifestState, latentStateSampler);\n",
    "\n",
    "// Build agent (using 'pomdp' object defined above fold)\n",
    "var agent = makePSRLAgent({ utility, priorBelief, alpha: 100 }, pomdp);\n",
    "\n",
    "var numEpisodes = 10\n",
    "var trajectories = simulatePSRL(startState, agent, numEpisodes);\n",
    "\n",
    "var concatAll = function(list) {\n",
    "  var inner = function (work, i) { \n",
    "    if (i < list.length-1) {\n",
    "      return inner(work.concat(list[i]), i+1) \n",
    "    } else {\n",
    "      return work;\n",
    "    }\n",
    "  }\n",
    "  return inner([], 0); \n",
    "};\n",
    "\n",
    "var badState = [[ { manifestState : { loc : \"break\" } } ]];\n",
    "\n",
    "var trajectories = map(function(t) { return t.concat(badState);}, trajectories);\n",
    "viz.gridworld(pomdp, {trajectory : concatAll(trajectories)});\n",
    "\n",
    "'''\n",
    "''' '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364.0036563071298"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "199.11/0.547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
