{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#x1F4D1; &nbsp; <span style=\"color:red\"> Reflections. Introduction To Reinforcement Learning. Lessons 9-10</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\">Links & Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reinforcement Learning: An Introduction http://incompleteideas.net/sutton/book/bookdraft2016aug.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "POMDP tutorials https://www.techfak.uni-bielefeld.de/~skopp/Lehre/STdKI_SS10/POMDP_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POMDP solution methods https://www.techfak.uni-bielefeld.de/~skopp/Lehre/STdKI_SS10/POMDP_solution.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Making under Uncertainty. MDPs and POMDPs http://web.stanford.edu/~mykel/pomdps.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POMDPs https://people.eecs.berkeley.edu/~pabbeel/cs287-fa13/slides/pomdps.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact Solutions of Interactive POMDPs Using Behavioral Equivalence\n",
    "https://www.cs.uic.edu/~piotr/papers/aamas06.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partially observable Markov decision process https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Survey of POMDP Solution Techniques https://www.cs.ubc.ca/~murphyk/Papers/pomdp.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Survey of POMDP Applications http://www.pomdp.org/papers/applications.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  &#x1F916; &nbsp;  <span style=\"color:red\"> Lesson 9. Partially Observable MDPs</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A **POMDP** models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Formally, a POMDP is a 7-tuple ${\\displaystyle (S,A,T,R,\\Omega ,O,\\gamma )}$, where\n",
    "\n",
    "- ${\\displaystyle S}$ is a set of states,\n",
    "- ${\\displaystyle A}$ is a set of actions,\n",
    "- ${\\displaystyle T}$ is a set of conditional transition probabilities between states,\n",
    "- ${\\displaystyle R:S\\times A\\to \\mathbb {R} }$ is the reward function.\n",
    "- ${\\displaystyle \\Omega }$  is a set of observations,\n",
    "- ${\\displaystyle O}$ is a set of conditional observation probabilities, and\n",
    "- ${\\displaystyle \\gamma \\in [0,1]}$ is the discount factor.\n",
    "\n",
    "At each time period, the environment is in some state ${\\displaystyle s\\in S}$. \n",
    "\n",
    "The agent takes an action ${\\displaystyle a\\in A}$, which causes the environment to transition to state ${\\displaystyle s'}$ with probability ${\\displaystyle T(s'\\mid s,a)}$. \n",
    "\n",
    "At the same time, the agent receives an observation ${\\displaystyle o\\in \\Omega }$  which depends on the new state of the environment with probability ${\\displaystyle O(o\\mid s',a)}$. \n",
    "\n",
    "Finally, the agent receives a reward equal to ${\\displaystyle R(s,a)}$. \n",
    "\n",
    "Then the process repeats. \n",
    "\n",
    "The goal is for the agent to choose actions at each time step that maximize its expected future discounted reward: ${\\displaystyle E\\left[\\sum _{t=0}^{\\infty }\\gamma ^{t}r_{t}\\right]}$ ^. \n",
    "\n",
    "The discount factor ${\\displaystyle \\gamma }$ determines how much immediate rewards are favored over more distant rewards. When ${\\displaystyle \\gamma =0}$ the agent only cares about which action will yield the largest expected immediate reward; when ${\\displaystyle \\gamma =1}$ the agent cares about maximizing the expected sum of future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POMDP ≡ Continuous-Space Belief MDP (don’t get to observe the state itself, instead get sensory measurements)\n",
    "\n",
    "- A belief state is a distribution over states; in belief state b, probability b(s) is assigned to being in s\n",
    "- Policies in POMDPs are mappings from belief states to actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing belief states**\n",
    "\n",
    "- Begin with some initial belief state b prior to any observations\n",
    "- Compute new belief state b' based on current belief state b, action a, and observation o\n",
    "- b'(s') = P(s'| o, a, b)\n",
    "- b'(s') ∝ P(o | s', a, b)P(s'| a, b)\n",
    "- b'(s') ∝ O(o | s', a)P(s'| a, b)\n",
    "- b'(s') ∝ O(o | s', a) ${\\sum_s}$ P(s'| a, b, s)P(s | a, b)\n",
    "- b'(s') ∝ O(o | s, a) ${\\sum_s}$ T(s'| s, a)b(s)\n",
    "- Kalman filter: exact update of the belief state for linear dynamical systems\n",
    "- Particle filter: approximate update for general systems\n",
    "- For this case only considering discrete state problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Exact solution algorithms**\n",
    "  - for general POMDP it is PSPACE-hard\n",
    "  - value iteration\n",
    "  -  policy iteration: policy evaluation and policy improvement\n",
    "  \n",
    "- **Offline methods**\n",
    "  - Offline POMDP methods involve doing all or most of the computing prior to execution\n",
    "  - Practical method generally find only approximate solutions\n",
    "  - Some methods output policy in terms of alpha-vectors, others as finite-state controllers\n",
    "  - Some methods involve leveraging a factored representation\n",
    "  - Very active area of research\n",
    "  - Examples:\n",
    "    - QMDP value approximation\n",
    "    - Fast informed bound (FIB) value approximation\n",
    "    - Point-based value iteration methods\n",
    "  \n",
    "- **Online methods**\n",
    "  - Online methods determine the optimal policy by planning from the current belief state\n",
    "  - Many online methods use a depth-first tree-based search up to some horizon\n",
    "  - The belief states reachable from the current state is typically small compared to the full belief space\n",
    "  - Time complexity is exponential in the search depth\n",
    "  - Heuristics and brand-and-bound techniques allow search space to be pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Industrial Applications\n",
    "  - Machine Maintenance\n",
    "  - Structural Inspection\n",
    "  - Elevator Control Policies\n",
    "  - Fishery Industry\n",
    "- Scientific Applications\n",
    "  - Autonomous Robots\n",
    "    - interplanetary rovers\n",
    "    - deep-space navigation\n",
    "    - bomb disposal\n",
    "    - land-mine clearing\n",
    "    - toxic waste clean-up\n",
    "    - radioactive material handling\n",
    "    - deep-ocean exploration\n",
    "    - sewage/drainage network inspection and repair\n",
    "  - Behavioral Ecology\n",
    "  - Machine Vision\n",
    "- Business Applications\n",
    "  - Distributed Database Queries\n",
    "  - Marketing\n",
    "  - Questionnaire Design\n",
    "  - Corporate Policy\n",
    "- Military Applications\n",
    "  - Moving Target Search and Identification\n",
    "  - Rescue\n",
    "  - Weapon Allocation\n",
    "- Social Applications\n",
    "  - Development of Teaching Strategies\n",
    "  - Health Care Policymaking\n",
    "  - Advanced Medical Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  &#x1F916; &nbsp;  <span style=\"color:red\"> Lesson 10. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
