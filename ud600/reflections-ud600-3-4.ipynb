{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# &#x1F4D1; &nbsp; <span style=\"color:red\"> Reflections. Introduction To Reinforcement Learning. Lessons 3-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   &#x1F916; &nbsp; <span style=\"color:red\">Links</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Analysis of Temporal-Difference Learning with Function Approximation\n",
    "http://www.mit.edu/~jnt/Papers/J063-97-bvr-td.pdf\n",
    "\n",
    "Reinforcement Learning\n",
    "http://www.cis.upenn.edu/~cis519/fall2015/lectures/14_ReinforcementLearning.pdf\n",
    "\n",
    "Reinforcement Learning & Monte Carlo Planning\n",
    "https://courses.cs.washington.edu/courses/csep573/12au/lectures/18-rl.pdf\n",
    "\n",
    "Reinforcement Learning\n",
    "http://isites.harvard.edu/fs/docs/icb.topic539621.files/lec5.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Forward View of TD(λ): \n",
    "https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node74.html\n",
    "\n",
    "True Online TD(λ): \n",
    "http://machinelearning.wustl.edu/mlpapers/paper_files/icml2014c1_seijen14.pdf\n",
    "http://wittawat.com/assets/talks/true_online_td.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-Free Prediction: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning: http://www.lsi.upc.es/~mmartin/Ag4-4x.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MathML: https://www.w3.org/TR/MathML/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://incompleteideas.net/sutton/book/bookdraft2016aug.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  &#x1F916; &nbsp;  <span style=\"color:red\"> Lesson 3. TD and Friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **reward hypothesis**:\n",
    "That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Dynamic Programming (DP)*** is based on the Bellman Equation and breaks down a problem into subproblems. Dividing a big task into smaller steps, this approach is depending on a perfect model of the environment.\n",
    "\n",
    "***Monte Carlo methods (MC)*** do not need a model of the learning environment. From experience in form of sequences of state-action-reward-samples they can approximate future rewards. However the methods only update after a complete sequence, when the final state is reached.\n",
    "\n",
    "***Temporal Difference (TD) methods*** combine both procedures - there is no need for a model of the learning\n",
    "environment and updates are available at each state of the incremental procedure. The method learns\n",
    "directly from the raw experience in a partially unknown system with each recorded sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping: update involves an estimate\n",
    "    \n",
    "- MC does not bootstrap\n",
    "- DP bootstraps\n",
    "- TD bootstraps\n",
    "\n",
    "Sampling: update samples an expectation\n",
    "\n",
    "- MC samples\n",
    "- DP does not sample\n",
    "- TD samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Model-based RL:  Learn a model, and use it to derive a controller.\n",
    "\n",
    "Model-free RL: Learn a controller without learning a model.\n",
    "\n",
    "- Q Learning: the function Q(i, a) provides the expected value of taking action a in state i. The Q value incorporates both the immediate reward and expected value of the next state reached. The Q function also determines a policy and a value function.\n",
    "\n",
    "Temporal difference method. \n",
    "\n",
    "- TD tries to combine the advantages of both the model-based and model-free approaches. Like the model-based approach, it does learn a transition model. Like the model-free approach, it provides a very quick decision procedure. TD also makes sense if the transition model is known, but the MDP is very hard to solve.\n",
    "- Rather than learn the Q function, TD learns the value function V (i). The idea is quite similar to Q learning. It is based on the fact that the value of a state i is equal to the expected immediate reward at i plus the expected future reward from the successor state j. Every time a transition is made from i to j, we receive a sample of both the immediate and future rewards. On transitioning from i to j and receiving reward r, the estimate for the value of i is updated according to the formula:\n",
    "  - V (i) ← (1 − α) V (i) + α(r + V (j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages and Disadvantages of MC vs. TD:\n",
    "\n",
    "- TD can learn before knowing the final outcome\n",
    "  - TD can learn online after every step\n",
    "  - MC must wait until end of episode before return is known\n",
    "- TD can learn without the final outcome\n",
    "  - TD can learn from incomplete sequences\n",
    "  - MC can only learn from complete sequences\n",
    "  - TD works in continuing (non-terminating) environments\n",
    "  - MC only works for episodic (terminating) environments  \n",
    "- MC has high variance, zero bias\n",
    "  - Good convergence properties (even with function approximation)\n",
    "  - Not very sensitive to initial value\n",
    "  - Very simple to understand and use\n",
    "- TD has low variance, some bias\n",
    "  - Usually more efficient than MC\n",
    "  - TD(0) converges to vπ(s) (but not always with function approximation)\n",
    "  - More sensitive to initial value\n",
    "- TD exploits Markov property\n",
    "  - Usually more efficient in Markov environments\n",
    "- MC does not exploit Markov property\n",
    "  - Usually more effective in non-Markov environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**TD(λ)** is a popular TD algorithm that combines basic TD learning with eligibility traces to further speed\n",
    "learning. The popularity of TD(λ) can be explained by its simple implementation, its low computational complexity, and its conceptually straightforward interpretation, given by its forward view.\n",
    "\n",
    "TD learning(MDP, π, γ)\n",
    "\n",
    " - Inputs: policy π, discount factor γ\n",
    " - Output: value function Vπ\n",
    " - Initialize V = 0, α according to schedule\n",
    " \n",
    "repeat\n",
    "\n",
    "1. initialize s\n",
    "2. while s 6∈ T do\n",
    "   - (a) take action a = π(s)\n",
    "   - (b) observe reward r and next state s′\n",
    "   - (c) V (s) = V (s) + α[r + γV (s′) − V (s)]\n",
    "   - (d) let s = s′\n",
    "3. decay α according to schedule\n",
    "\n",
    "forever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate parameter ${\\alpha}$\n",
    "\n",
    "- ${\\alpha}$ is used for weighting different experiences\n",
    "- In stationary environments:\n",
    "$${\\alpha (s) = \\frac {1} {number \\ of \\ visits \\ to \\ state \\ s}}$$\n",
    "\n",
    "  - In this case, the Q and V values are the exact arithmetic average of the experiences\n",
    "  \n",
    "- In non-stationary environments:\n",
    "  -  takes a constant value (usually on the range 0,3..0,5)\n",
    "- Constant values decay relative influence of past experiences\n",
    "- As higher the value, higher the learning (more influence of recent experiences in the estimations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *forward view* of TD(λ) (Sutton & Barto, 1998) is that the estimate at each time step is moved toward an update target known as as the λ-return; the corresponding algorithm is known as the λ-return algorithm. The λ-return is an estimate of the expected return based on both subsequent rewards and the expected-return estimates at subsequent states, with λ determining the precise way these are combined. The forward view is useful primarily for understanding\n",
    "the algorithm theoretically and intuitively.\n",
    "\n",
    "The *backward view* of TD(λ) provides a causal, incremental mechanism for approximating the forward view and, in the off-line case, for achieving it exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD(0) algorithms\n",
    "  - Q-learning\n",
    "  - Sarsa\n",
    "\n",
    "- TD(1) is roughly equivalent to every-visit Monte-Carlo\n",
    "  - Error is accumulated online, step-by-step\n",
    "  - If value function is only updated offline at end of episode then total update is exactly the same as MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *true online* TD(λ) is a new variant of TD(λ) allowing exact online updates with the same computational complexity as classical TD(λ).\n",
    "\n",
    "Online updates\n",
    "\n",
    "- TD(λ) updates are applied online at each step within episode\n",
    "- Forward and backward-view TD(λ) are slightly different\n",
    "- NEW: Exact online TD(λ) achieves perfect equivalence\n",
    "- By using a slightly different form of eligibility trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement Learning**: \n",
    "\n",
    "- model-based \n",
    "- value-based (TD) \n",
    "- policy-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  &#x1F916; &nbsp;  <span style=\"color:red\"> Lesson 4. Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- policy evaluation or **prediction problem**\n",
    "  - estimating the value function  for a given policy\n",
    "- **control problem** (finding an optimal policy)\n",
    "  - some variation of generalized policy iteration (GPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The **Bellman-Equation** \n",
    "\n",
    "- expresses the relationship between a value of a state and the value of a successor state, \n",
    "- calculates the value of a state by considering all available options and assessing each by its likelihood of appearance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let $V^{\\pi}(s)$ be the state-value function at the state $s$ using the policy $\\pi$ and $Q^{\\pi}(s,a)$ - the q-value function at the state $s$ taking the action $a$ using the policy ${\\pi}$ (with the reward function R and the transition function P)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$V^{\\pi}(s) = \\sum_a {\\pi(a,s)} \\sum_{s'}{P_{ss'}^a \\ [R_{ss'}^a + \\gamma V^{\\pi}(s')]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$Q^{\\pi}(s,a) = \\sum_{s'}{P_{ss'}^a \\ [R_{ss'}^a + \\gamma V^{\\pi}(s')]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finite MDPs there is always at least one optimal policy ${\\pi^*}$ that is better than or equal to all other policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value functions partially order the policies,\n",
    "\n",
    "- but at least one optimal policy exists, and\n",
    "- all optimal policies have the same value function V*.\n",
    "\n",
    "The optimal value function can be interpreted as the total reward received by an agent behaving optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V^∗(s)= \\max_π V^π(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal policies also share the same optimal action-value function $Q^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q^∗(s,a)= \\max_π Q^π(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Generalized MDPs Examples**:\n",
    "\n",
    "- alternating Markov games\n",
    "- discounted expected-reward MDPs \n",
    "- risk-sensitive MDPs\n",
    "- exploration-sensitive MDPs\n",
    "- etc."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
